

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Pre-implemented Rewards &mdash; PDEControlGym 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/css/pdecg_theme.css?v=c04f4f2d" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=d45e8c67"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Custom Rewards" href="customrewards.html" />
    <link rel="prev" title="2D Custom Environments" href="../custom_environments/2dbaseenvironment.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            PDEControlGym
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide/quickstart.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide/tutorials.html">Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environments</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../environments/hyperbolic-1d.html">Transport 1D PDE</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments/parabolic-1d.html">Reaction-Diffusion 1D PDE</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments/navierstokes2d.html">Navier-Stokes 2D PDE</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments/Trafficarz1d.html">Traffic ARZ 1D PDE</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Custom Environments</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../custom_environments/1dbaseenvironment.html">1D Custom Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_environments/2dbaseenvironment.html">2D Custom Environments</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Utilities</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Pre-implemented Rewards</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#normalized-reward">Normalized Reward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pde_control_gym.src.rewards.NormReward"><code class="docutils literal notranslate"><span class="pre">NormReward</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#pde_control_gym.src.rewards.NormReward.reward"><code class="docutils literal notranslate"><span class="pre">NormReward.reward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#tuned-1d-custom-reward">Tuned 1D Custom Reward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pde_control_gym.src.rewards.TunedReward1D"><code class="docutils literal notranslate"><span class="pre">TunedReward1D</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#pde_control_gym.src.rewards.TunedReward1D.reward"><code class="docutils literal notranslate"><span class="pre">TunedReward1D.reward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#ns-reward">NS Reward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pde_control_gym.src.rewards.NSReward"><code class="docutils literal notranslate"><span class="pre">NSReward</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#pde_control_gym.src.rewards.NSReward.reward"><code class="docutils literal notranslate"><span class="pre">NSReward.reward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="customrewards.html">Custom Rewards</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PDEControlGym</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Pre-implemented Rewards</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/utils/preimplementedrewards.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="pre-implemented-rewards">
<span id="module-pde_control_gym.src.rewards"></span><span id="preimplementedrewards"></span><h1>Pre-implemented Rewards<a class="headerlink" href="#pre-implemented-rewards" title="Link to this heading"></a></h1>
<p>This gym comes with a series of pre-implemented reward functions that are detailed in the documentation here. See <a class="reference external" href="../utils/customrewards.html">this page</a> for implementing your own custom rewards either in the gym or using an outside function as in <a class="reference external" href="../guide/tutorials.html">the tutorial doc</a>.</p>
<section id="normalized-reward">
<h2>Normalized Reward<a class="headerlink" href="#normalized-reward" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pde_control_gym.src.rewards.NormReward">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pde_control_gym.src.rewards.</span></span><span class="sig-name descname"><span class="pre">NormReward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'2'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">horizon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'temporal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncate_penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">terminate_reward</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">t_horizon_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">extras</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pde_control_gym/src/rewards/norm_reward.html#NormReward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pde_control_gym.src.rewards.NormReward" title="Link to this definition"></a></dt>
<dd><p>This reward offers <span class="math notranslate nohighlight">\(L_1, L_2, L_\infty\)</span> norms that can be implemented in a vairety of different ways according to the parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>nt</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – The number of maximum timesteps for the episode simulation. No default: Error is thrown if not specified.</p></li>
<li><p><strong>norm</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></span>) – Norm to use. Accepts “1”, “2”, “inf” corresponding to the norm to use. Default is “2”.</p></li>
<li><p><strong>horizon</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></span>) – Designates how to compute the norm. “temporal” indicates to indicate the norm of the current state. “differential” indicates to compute the normed difference between this state and the last state. “t-horizon” allows the norm to be computed for the average of last <code class="docutils literal notranslate"><span class="pre">t-horion-length</span></code> steps. Default is “temporal”.</p></li>
<li><p><strong>truncate_penalty</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></span>) – Allows the user to specify a penalty reward for each remaining timestep in case the episode is ended early. Default is <span class="math notranslate nohighlight">\(-1e4\)</span>.</p></li>
<li><p><strong>terminate_reward</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></span>) – Allows the user to add a reward for reaching the full length of the episode: Default is <span class="math notranslate nohighlight">\(1e2\)</span>.</p></li>
<li><p><strong>t_horizon_length</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Allows the user to set the length to be average over in the case of <code class="docutils literal notranslate"><span class="pre">t-horizon</span></code> approach for the <code class="docutils literal notranslate"><span class="pre">horizon</span></code> parameter. Default is <span class="math notranslate nohighlight">\(5\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pde_control_gym.src.rewards.NormReward.reward">
<span class="sig-name descname"><span class="pre">reward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">uVec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">terminate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pde_control_gym/src/rewards/norm_reward.html#NormReward.reward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pde_control_gym.src.rewards.NormReward.reward" title="Link to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>uVec</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></span>) – (required) This is the solution vector of the PDE of which to compute the reward on.</p></li>
<li><p><strong>time_index</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – (required) This is the time at which to compute the reward. (Given in terms of index of uVec).</p></li>
<li><p><strong>terminate</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]</span>) – States whether the episode is the terminal episode.</p></li>
<li><p><strong>truncate</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]</span>) – States whether the epsiode is truncated, or ending early.</p></li>
<li><p><strong>action</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]</span>) – Ignored in this reward - needed to inherit from base reward class.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="tuned-1d-custom-reward">
<h2>Tuned 1D Custom Reward<a class="headerlink" href="#tuned-1d-custom-reward" title="Link to this heading"></a></h2>
<p>This implements the reward as used in the <a class="reference external" href="https://google.com">benchmark paper</a> which is defined as:</p>
<div class="math notranslate nohighlight">
\begin{eqnarray}
  Reward(t) = \begin{cases}
     \text{truncate_penalty}*(T-t) &amp; \text{truncate}=\text{True} \\
    \text{terminate_reward} - \sum_{i=0}^{nt}|u(-1, i)| / 1000
     - \|u(x, T)\|_{L_2} &amp; \text{terminate}=\text{True} \\ &amp; \text{and} \\ &amp;  \|u(x, T)\|_{L_2} &lt; 20 \\
     \|u(x, t-dt*1/\text{control_sample_rate}\|_{L_2} - \|u(x, t)\|_{L_2} &amp; \text{Otherwise}
     \end{cases}
\end{eqnarray}</div><p>where <span class="math notranslate nohighlight">\(u(x, t)\)</span> is the solution vector, T is final simulation time, <cite>control_sample_rate</cite> is the rate at which the controller is resampled (Default=0.01), and <span class="math notranslate nohighlight">\(\|u(x, t)\|_{L_2}\)</span> represents the <span class="math notranslate nohighlight">\(L_2\)</span> norm at time <span class="math notranslate nohighlight">\(t\)</span> over <span class="math notranslate nohighlight">\(x\)</span>.</p>
<dl class="py class">
<dt class="sig sig-object py" id="pde_control_gym.src.rewards.TunedReward1D">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pde_control_gym.src.rewards.</span></span><span class="sig-name descname"><span class="pre">TunedReward1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncate_penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">terminate_reward</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pde_control_gym/src/rewards/tuned_reward_1d.html#TunedReward1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pde_control_gym.src.rewards.TunedReward1D" title="Link to this definition"></a></dt>
<dd><p>This is a custom reward used for successfully implementing the model-free boundary controller for the 1D environments as given in the <a class="reference external" href="https://google.com">paper</a> documenting the benchmark.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>nt</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – The number of maximum timesteps for the episode simulation. No default: Error is thrown if not specified.</p></li>
<li><p><strong>truncate_penalty</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></span>) – Allows the user to specify a penalty reward for each remaining timestep in case the episode is ended early. Default is <span class="math notranslate nohighlight">\(-1e4\)</span>.</p></li>
<li><p><strong>terminate_reward</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></span>) – Allows the user to add a reward for reaching the full length of the episode: Default is <span class="math notranslate nohighlight">\(1e2\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pde_control_gym.src.rewards.TunedReward1D.reward">
<span class="sig-name descname"><span class="pre">reward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">uVec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">terminate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">control_sample_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pde_control_gym/src/rewards/tuned_reward_1d.html#TunedReward1D.reward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pde_control_gym.src.rewards.TunedReward1D.reward" title="Link to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>uVec</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></span>) – (required) This is the solution vector of the PDE of which to compute the reward on.</p></li>
<li><p><strong>time_index</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – (required) This is the time at which to compute the reward. (Given in terms of index of uVec).</p></li>
<li><p><strong>terminate</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]</span>) – States whether the episode is the terminal episode.</p></li>
<li><p><strong>truncate</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]</span>) – States whether the epsiode is truncated, or ending early.</p></li>
<li><p><strong>action</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]</span>) – Ignored in this reward - needed to inherit from base reward class.</p></li>
<li><p><strong>control_sample_rate</strong> (<em>float</em><em> | </em><em>None</em>)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="ns-reward">
<h2>NS Reward<a class="headerlink" href="#ns-reward" title="Link to this heading"></a></h2>
<p>This implements the reward to track the reference trajectory as well as minimizing the control action loss which is defined as:</p>
<div class="math notranslate nohighlight">
\begin{eqnarray}
  Reward(t) = -\frac{1}{2} \|s' - s_{ref}\|^2 - \frac{\gamma}{2} \| a - a_{ref}\|^2
\end{eqnarray}</div><p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is the coefficient for the control cost.</p>
<dl class="py class">
<dt class="sig sig-object py" id="pde_control_gym.src.rewards.NSReward">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pde_control_gym.src.rewards.</span></span><span class="sig-name descname"><span class="pre">NSReward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pde_control_gym/src/rewards/ns_reward.html#NSReward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pde_control_gym.src.rewards.NSReward" title="Link to this definition"></a></dt>
<dd><p>This is a reward aiming at tracking the desired trajectory while minimizing the action cost.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>gamma</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></span>) – coefficient for the action cost</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pde_control_gym.src.rewards.NSReward.reward">
<span class="sig-name descname"><span class="pre">reward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">uVec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">U_ref</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_ref</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pde_control_gym/src/rewards/ns_reward.html#NSReward.reward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pde_control_gym.src.rewards.NSReward.reward" title="Link to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>uVec</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></span>) – (required) This is the difference of the vector of PDE and tracking trajectory.</p></li>
<li><p><strong>action</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>]</span>) – (required) control actions</p></li>
<li><p><strong>time_index</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – (required) time index of the simulation</p></li>
<li><p><strong>U_ref</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>]</span>) – (required) reference trajectory</p></li>
<li><p><strong>action_ref</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>]</span>) – (required) reference action or action sequences</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../custom_environments/2dbaseenvironment.html" class="btn btn-neutral float-left" title="2D Custom Environments" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="customrewards.html" class="btn btn-neutral float-right" title="Custom Rewards" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, PDEContRoLGym.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>